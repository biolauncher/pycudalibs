pycudalibs - simple but flexible interfaces to CUDA BLAS and CULA LAPACK libraries.  
(C) Copyright 2009-2011 Simon Beaumont - Model Sciences Ltd.
----------------------------------------------------------------------------------

We are releasing this code under LGPL3 - see copyright.txt, COPYING, COPYING.LESSER

This branch integrates the CULA LAPACK libraries and leverage the
performance improvements to CULA BLAS implementations.  CULA is
proprietary (though very reasonbly priced for research) and there is
free version with single precision functions. See:
http://www.culatools.com/

The philopsophy of this implementation is to provide cuda matrices
(and vectors) along the lines of numpy that use device functions and
kernels to implement their methods for general linear algebra, data
analysis and machine learning tasks. It should be possible to create
useful algorithms by composing array functions without incurring host
to device and device host memory transfer costs for intermediate
results or "in flight" data.

For more general utility linking the underlying BLAS/LAPACK
implementation to the CULA link libraries (which are adaptive to a
tunable job size) is more transparent and can also fail over to a high
performance CPU based implementation e.g. Intel's MKL (math kernel
library) which will leverage multi-core speedups.

More sophisticated Python/CUDA programmers wanting to enjoy the
whole CUDA, Open/CL experience will also benefit from the full monty
offered by the excellent PyCUDA package.

In short you probably don't need this.

Synopsis: 
import cunumpy 
import gpu
 
Tested on:
OS X 10.5.*, CUDA 2.0-1 and Python 2.5-6
OS X 10.6.*, CUDA 4.0   and Python 2.7.1
UBUNTU 11.04 (kernel 2.6.38) x86_64 CUDA 4.0 EPD 7.1.1

Coming soon:
Bespoke kernel integrations for some specialised functions.
FFT
________

